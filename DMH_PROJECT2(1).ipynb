{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd8704df",
   "metadata": {},
   "source": [
    "## AWS Info\n",
    "\n",
    " - AWS Access Key ID: ASIARRKSDW2JSOLKVOH6\n",
    " - AWS Secret Access Key: 9fck1/Iw5aEh/U4MiF7dZIEI+Xh0P/muJs5erFfM\n",
    " - AWS Session Token: FwoGZXIvYXdzEOz//////////wEaDDnRk7ABTv8YQeRX5CLVAeUl9P1XR7kUd0BXCF0jSvect4ZodvPSo4bX1NKpivLQJu/FWWdz3KnMASdSxlLXnV8qS4sQVyL8RLqQ+4JNo9GD12o5jBQyRW1KjADthsWI3K2aijHUQyX+/VF1crETvZzoeNcMNsf1l651jt9tuNH3yD6LMjifohei2DUhWXjU+sDKSoMynpBOn5HbQZXEpRUjUu3TViteg7Q2tLIX9mO4zhz39xHZK9VYTzKk7aecF8ERaLIasGbfnx3m8nn2FPZJzXI4AzPI3eR1YlB8tug5LLkBHyjsqZKtBjItgFxVUhKAGBcxwJhaH2pFqAJKxe9er6h7o8YvNJOWhr/DPtHZbqkt6GQ3juqp\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e31fe9d5",
   "metadata": {},
   "source": [
    "# readme\n",
    "# Data Engineering with AWS: Project Cloud Data Warehouse\n",
    "\n",
    "## Purpose\n",
    "I am a Data Engineer for a startup called Sparkify. Sparkify's is rapidly scaling their user base and song data base to service these new users. To accomodate this, we are moving their data into the cloud. As part of this process, I have been tasked with creating an ETL pipeline to move data from an S3 AWS bucket, stage it in Reshift, and transofrm it into tables for analytical purposes.\n",
    "\n",
    "\n",
    "## Database Schema and ETL Pipeline Design\n",
    "\n",
    "### Database Schema\n",
    "I am going to create two staging tables:\n",
    "\n",
    "> staging_events: will source from JSON logs and contain dimenionson users \n",
    "    - artist\n",
    "    - auth\n",
    "    - firstName\n",
    "    - gender\n",
    "    - itemInSession\n",
    "    - lastName\n",
    "    - length\n",
    "    - level\n",
    "    - location\n",
    "    - method\n",
    "    - page\n",
    "    - registration\n",
    "    - sessionId\n",
    "    - song\n",
    "    - status\n",
    "    - ts\n",
    "    - userAgent\n",
    "    - userId\n",
    "    \n",
    " > staging_songs: will source from JSON logs and contain dimenionson songs \n",
    "     - num_songs\n",
    "     - artist_latitude\n",
    "     - artist_longitude\n",
    "     - artist_location\n",
    "     - artist_name\n",
    "     - song_id\n",
    "     - title\n",
    "     - year\n",
    "     - duration\n",
    "     - artist_id\n",
    "     \n",
    "I will also create a fact table.\n",
    "\n",
    "> songplays: data about song plays\n",
    "    - songplay_id (PK)\n",
    "    - start_time\n",
    "    - user_id\n",
    "    - level\n",
    "    - song_id\n",
    "    - artist_id\n",
    "    - session_id\n",
    "    - duration\n",
    "    - user_agent\n",
    "\n",
    "Finally, the database will have four dimension tables.\n",
    "\n",
    "> users: data on users\n",
    "    - user_id (PK)\n",
    "    - first_name\n",
    "    - last_name\n",
    "    - gender\n",
    "    - level\n",
    "    \n",
    " > songs: data on the songs in database\n",
    "     - song_id (PK)\n",
    "     - title\n",
    "     - artist_id\n",
    "     - year\n",
    "     - duration\n",
    "     \n",
    "> artist: data on artists in data\n",
    "    - artist_id (PK)\n",
    "    - name\n",
    "    - location\n",
    "    - latitude\n",
    "    - longitude\n",
    "    \n",
    " > time: data on when songs were played\n",
    "     - start_time (PK)\n",
    "     - hour\n",
    "     - day\n",
    "     - week\n",
    "     - month\n",
    "     - year\n",
    "     - weekday\n",
    "     \n",
    "     \n",
    "### ETL\n",
    "All code will be aggregated into a single project called main.ipynb\n",
    "\n",
    "etl.py will load data from S3 to the staging tables and then to the analytical tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37b260bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dhw.cgf\n",
    "#[CLUSTER]\n",
    "HOST='redshift-cluster-1.cspmdnqhgoxd.us-east-1.redshift.amazonaws.com:5439/dev'\n",
    "DB_NAME='dev'\n",
    "DB_USER='awsuser'\n",
    "DB_PASSWORD='TESTtest1'\n",
    "DB_PORT=5439\n",
    "\n",
    "#[IAM_ROLE]\n",
    "#ARN='arn:aws:redshift:us-east-1:105935779475:namespace:c7bafe03-bf5e-4f42-86a9-1bd6201b7483'\n",
    "ARN='arn:aws:iam::105935779475:role/myRedshiftRole'\n",
    "\n",
    "#[S3]\n",
    "LOG_DATA='s3://udacity-dend/log_data'\n",
    "LOG_JSONPATH='s3://udacity-dend/log_json_path.json'\n",
    "SONG_DATA='s3://udacity-dend/song_data'\n",
    "\n",
    "#[AWS]\n",
    "KEY='AKIARRKSDW2JVFP3I4UO'\n",
    "SECRET='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'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70d7f4a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSectionError",
     "evalue": "No section: 'IAM'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSectionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_34920/2412064287.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dwh.cfg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mDWH_ROLE_ARN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"IAM\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"ROLE_ARN\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mLOG_DATA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"S3\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"LOG_DATA\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mLOG_JSONPATH\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"S3\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"LOG_JSONPATH\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\configparser.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, section, option, raw, vars, fallback)\u001b[0m\n\u001b[0;32m    779\u001b[0m         \"\"\"\n\u001b[0;32m    780\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 781\u001b[1;33m             \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_unify_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msection\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    782\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mNoSectionError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    783\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mfallback\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0m_UNSET\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\configparser.py\u001b[0m in \u001b[0;36m_unify_values\u001b[1;34m(self, section, vars)\u001b[0m\n\u001b[0;32m   1150\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0msection\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdefault_section\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1152\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mNoSectionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msection\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1153\u001b[0m         \u001b[1;31m# Update with the entry specific variables\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m         \u001b[0mvardict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNoSectionError\u001b[0m: No section: 'IAM'"
     ]
    }
   ],
   "source": [
    "# sql_queries.py\n",
    "import configparser\n",
    "\n",
    "\n",
    "# CONFIG\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dwh.cfg')\n",
    "\n",
    "DWH_ROLE_ARN = config.get(\"IAM\",\"ROLE_ARN\")\n",
    "LOG_DATA = config.get(\"S3\",\"LOG_DATA\")\n",
    "LOG_JSONPATH = config.get(\"S3\", \"LOG_JSONPATH\")\n",
    "SONG_DATA = config.get(\"S3\", \"SONG_DATA\")\n",
    "\n",
    "# DROP TABLES\n",
    "\n",
    "staging_events_table_drop = \"DROP TABLE IF EXISTS staging_events;\"\n",
    "staging_songs_table_drop = \"DROP TABLE IF EXISTS staging_songs;\"\n",
    "songplay_table_drop = \"DROP TABLE IF EXISTS songplays;\"\n",
    "user_table_drop = \"DROP TABLE IF EXISTS users;\"\n",
    "song_table_drop = \"DROP TABLE IF EXISTS songs;\"\n",
    "artist_table_drop = \"DROP TABLE IF EXISTS artists;\"\n",
    "time_table_drop = \"DROP TABLE IF EXISTS time;\"\n",
    "\n",
    "# CREATE TABLES\n",
    "\n",
    "staging_events_table_create= (\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS staging_events (\n",
    "            artist VARCHAR,\n",
    "            auth VARCHAR,\n",
    "            firstName VARCHAR,\n",
    "            gender CHAR(1),\n",
    "            itemInSession INTEGER,\n",
    "            lastName VARCHAR,\n",
    "            length FLOAT,\n",
    "            level VARCHAR,\n",
    "            location TEXT,\n",
    "            method VARCHAR,\n",
    "            page VARCHAR,\n",
    "            registration FLOAT,\n",
    "            sessiond INTEGER,\n",
    "            song VARCHAR,\n",
    "            status INTEGER,\n",
    "            ts BIGINT,\n",
    "            userAgent TEXT,\n",
    "            userId INTEGER);\n",
    "\"\"\")\n",
    "\n",
    "staging_songs_table_create = (\"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS staging_songs (\n",
    "        num_songs INTEGER,\n",
    "        artist_id VARCHAR,\n",
    "        artist_latitude FLOAT,\n",
    "        artist_longitude FLOAT,\n",
    "        artist_location TEXT,\n",
    "        artist_name VARCHAR,\n",
    "        song_id VARCHAR,\n",
    "        title VARCHAR,\n",
    "        duration fLOAT,\n",
    "        year INTEGER);\n",
    "\"\"\")\n",
    "\n",
    "songplay_table_create = (\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS songplays (\n",
    "        songplay_id INTEGER IDENTITY(0,1) NOT NULL PRIMARY KEY,\n",
    "        start_time TIMESTAMP,\n",
    "        user_id INTEGER,\n",
    "        level VARCHAR,\n",
    "        song_id VARCHAR,\n",
    "        artist_id VARCHAR,\n",
    "        session_id INTEGER,\n",
    "        location TEXT,\n",
    "        user_agent TEXT);\n",
    "\"\"\")\n",
    "\n",
    "user_table_create = (\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS users (\n",
    "        user_id  INTEGER NOT NULL PRIMARY KEY,\n",
    "        first_name VARCHAR,\n",
    "        last_name VARCHAR,\n",
    "        gender CHAR(1),\n",
    "        level VARCHAR);\n",
    "\"\"\")\n",
    "\n",
    "song_table_create = (\"\"\"\n",
    "     CREATE TABLE IF NOT EXISTS songs (\n",
    "        song_id VARCHAR NOT NULL PRIMARY KEY,\n",
    "        title VARCHAR,\n",
    "        artist_id VARCHAR,\n",
    "        year INT,\n",
    "        duration FLOAT);\n",
    "\"\"\")\n",
    "\n",
    "artist_table_create = (\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS artists (\n",
    "        artist_id VARCHAR NOT NULL PRIMARY KEY,\n",
    "        name VARCHAR,\n",
    "        location TEXT ,\n",
    "        latitude FLOAT ,\n",
    "        longitude FLOAT);\n",
    "\"\"\")\n",
    "\n",
    "time_table_create = (\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS time (\n",
    "        start_time TIMESTAMP NOT NULL PRIMARY KEY,\n",
    "        hour INTEGER,\n",
    "        day INTEGER,\n",
    "        week INTEGER,\n",
    "        month INTEGER,\n",
    "        year INTEGER,\n",
    "        weekday VARCHAR);\n",
    "\"\"\")\n",
    "\n",
    "# STAGING TABLES\n",
    "\n",
    "staging_events_copy = (\"\"\"\n",
    "    copy staging_events \n",
    "    from {} \n",
    "    credentials 'aws_iam_role={}' \n",
    "    format as json {} \n",
    "    compupdate off \n",
    "    region 'us-west-2';\n",
    "\"\"\").format(LOG_DATA, DWH_ROLE_ARN, LOG_JSONPATH)\n",
    "\n",
    "staging_songs_copy = (\"\"\"\n",
    "    copy staging_songs \n",
    "    from {} \n",
    "    credentials 'aws_iam_role={}' \n",
    "    format as json 'auto' \n",
    "    compupdate off \n",
    "    region 'us-west-2';\n",
    "\"\"\").format(SONG_DATA, DWH_ROLE_ARN)\n",
    "\n",
    "# FINAL TABLES\n",
    "\n",
    "songplay_table_insert = (\"\"\"\n",
    "    INSERT INTO songplays (\n",
    "        start_time, \n",
    "        user_id, \n",
    "        level,\n",
    "        song_id, \n",
    "        artist_id, \n",
    "        session_id, \n",
    "        location, \n",
    "        user_agent\n",
    "    ) \n",
    "    SELECT \n",
    "        timestamp 'epoch' + se.ts/1000 * interval '1 second', \n",
    "        se.userId, \n",
    "        se.level, \n",
    "        ss.song_id, \n",
    "        ss.artist_id, \n",
    "        se.sessionId, \n",
    "        se.location, \n",
    "        se.userAgent\n",
    "    FROM staging_events se \n",
    "    JOIN staging_songs ss ON (se.song = ss.title AND se.artist = ss.artist_name)\n",
    "    WHERE se.page = 'NextSong';\n",
    "\"\"\")\n",
    "\n",
    "user_table_insert = (\"\"\"\n",
    "    INSERT INTO users (\n",
    "        user_id, \n",
    "        first_name, \n",
    "        last_name, \n",
    "        gender, \n",
    "        level\n",
    "    ) \n",
    "    SELECT \n",
    "        DISTINCT userId, \n",
    "        firstName, \n",
    "        lastName, \n",
    "        gender, \n",
    "        level\n",
    "    FROM staging_events\n",
    "    WHERE page = 'NextSong' AND userId IS NOT NULL\n",
    "\"\"\")\n",
    "\n",
    "song_table_insert = (\"\"\"\n",
    "    INSERT INTO songs (\n",
    "        song_id, \n",
    "        title, \n",
    "        artist_id, \n",
    "        year, \n",
    "        duration\n",
    "    )\n",
    "    SELECT \n",
    "        DISTINCT song_id, \n",
    "        title, \n",
    "        artist_id, \n",
    "        year, \n",
    "        duration\n",
    "    FROM staging_songs\n",
    "    WHERE song_id IS NOT NULL\n",
    "\"\"\")\n",
    "\n",
    "artist_table_insert = (\"\"\"\n",
    "    INSERT INTO artists (\n",
    "        artist_id, \n",
    "        name, \n",
    "        location, \n",
    "        latitude, \n",
    "        longitude\n",
    "    )\n",
    "    SELECT \n",
    "        DISTINCT artist_id, \n",
    "        artist_name, \n",
    "        artist_location, \n",
    "        artist_latitude, \n",
    "        artist_longitude\n",
    "    FROM staging_songs\n",
    "    WHERE artist_id IS NOT NULL\n",
    "\"\"\")\n",
    "\n",
    "time_table_insert = (\"\"\"\n",
    "    INSERT INTO time (\n",
    "        start_time, \n",
    "        hour, \n",
    "        day, \n",
    "        week, \n",
    "        month, \n",
    "        year, \n",
    "        weekday\n",
    "    )\n",
    "    SELECT \n",
    "        DISTINCT start_time, \n",
    "        EXTRACT(hour from start_time), \n",
    "        EXTRACT(day from start_time), \n",
    "        EXTRACT(week from start_time), \n",
    "        EXTRACT(month from start_time), \n",
    "        EXTRACT(year from start_time), \n",
    "        EXTRACT(weekday from start_time)\n",
    "    FROM songplays\n",
    "\"\"\")\n",
    "\n",
    "# QUERY LISTS\n",
    "\n",
    "create_table_queries = [staging_events_table_create, staging_songs_table_create, songplay_table_create, user_table_create, song_table_create, artist_table_create, time_table_create]\n",
    "drop_table_queries = [staging_events_table_drop, staging_songs_table_drop, songplay_table_drop, user_table_drop, song_table_drop, artist_table_drop, time_table_drop]\n",
    "copy_table_queries = [staging_events_copy, staging_songs_copy]\n",
    "insert_table_queries = [songplay_table_insert, user_table_insert, song_table_insert, artist_table_insert, time_table_insert]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9395455f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sql_queries'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_34920/3687813185.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfigparser\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpsycopg2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msql_queries\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcopy_table_queries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minsert_table_queries\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sql_queries'"
     ]
    }
   ],
   "source": [
    "# etl.py\n",
    "import configparser\n",
    "import psycopg2\n",
    "from sql_queries import copy_table_queries, insert_table_queries\n",
    "\n",
    "\n",
    "def load_staging_tables(cur, conn):\n",
    "    for query in copy_table_queries:\n",
    "        cur.execute(query)\n",
    "        conn.commit()\n",
    "\n",
    "\n",
    "def insert_tables(cur, conn):\n",
    "    for query in insert_table_queries:\n",
    "        cur.execute(query)\n",
    "        conn.commit()\n",
    "\n",
    "\n",
    "def main():\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read('dwh.cfg')\n",
    "\n",
    "    conn = psycopg2.connect(\"host={} dbname={} user={} password={} port={}\".format(*config['CLUSTER'].values()))\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    load_staging_tables(cur, conn)\n",
    "    insert_tables(cur, conn)\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d51594",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_tables.py\n",
    "import configparser\n",
    "import psycopg2\n",
    "from sql_queries import create_table_queries, drop_table_queries\n",
    "\n",
    "\n",
    "def drop_tables(cur, conn):\n",
    "    for query in drop_table_queries:\n",
    "        cur.execute(query)\n",
    "        conn.commit()\n",
    "\n",
    "\n",
    "def create_tables(cur, conn):\n",
    "    for query in create_table_queries:\n",
    "        cur.execute(query)\n",
    "        conn.commit()\n",
    "\n",
    "\n",
    "def main():\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read('dwh.cfg')\n",
    "\n",
    "    conn = psycopg2.connect(\"host={} dbname={} user={} password={} port={}\".format(*config['CLUSTER'].values()))\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    drop_tables(cur, conn)\n",
    "    create_tables(cur, conn)\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187282e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
